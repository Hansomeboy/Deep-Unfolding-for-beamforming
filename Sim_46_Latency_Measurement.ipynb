{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Functions"
      ],
      "metadata": {
        "id": "-iXketwgKl3t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "TuOhMYZ7KePr"
      },
      "outputs": [],
      "source": [
        "from torch.cuda import device_of\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import time\n",
        "\n",
        "def set_general_params(n_0 = 1, p_s = 1, B = 4, K = 4, P = 4, N = 2, L = 2,\n",
        "                       T = 5,epochs=15, J=10,X_is_variable = False,\n",
        "                       W_is_phase_only = True, W_is_block_diag = True,\n",
        "                       train_size = 1500, batch_size = 20, pga_iters=400,\n",
        "                       optimizer_learning_rate=0.03):\n",
        "    \"\"\"\n",
        "    This function sets the general parameters of the network.\n",
        "     used in order to save space in the code when running multiple simulations.\n",
        "    :param n_0: noise power\n",
        "    :param p_s: signal power\n",
        "    :param B: number of frequency bins\n",
        "    :param K: number of users to be served\n",
        "    :param P: number of panels in the base station\n",
        "    :param N: number of antennas in each panel\n",
        "    :param L: number of outputs in each panel\n",
        "    :param T: number of inputs to the CPU\n",
        "    :param epochs: number of epochs in the training\n",
        "    :param J: number of iteration to be unfolded , usually 10.\n",
        "    :param X_is_variable: if True, X is a variable in the network. usually False.\n",
        "    :param W_is_phase_only: if True, W is a phase matrix. usually True.\n",
        "    :param W_is_block_diag: if True, W is a block diagonal matrix. usually True.\n",
        "    :param train_size: number of samples in the training set.\n",
        "    :param batch_size: batch size in the training part.\n",
        "    :param pga_iters: number of iterations in the PGA algorithm to be compared with. in order of 300-400.\n",
        "    :param optimizer_learning_rate: learning rate of the optimizer. denoted as eta in the paper.\n",
        "    :return: all the parameters.\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    M = P * N  # Total number of antennas in the Base station\n",
        "    snr = p_s / n_0\n",
        "    T_opt = max(math.floor(M * (K - L) / K + 1), K)  # minimal number of inputs to the CPU that allows loss-less processing. as described in \"Trade-Offs in Decentralized Multi-Antenna Architectures: The WAX Decomposition\".\n",
        "    if T==5: # build the A matrix using the example given in \"Trade-Offs in Decentralized Multi-Antenna Architectures: The WAX Decomposition\"\n",
        "        A = torch.empty((L * P, T))\n",
        "        A[:T, :] = torch.eye(T)\n",
        "        A[T:, :3] = torch.eye(3)\n",
        "        A[T:, 3:] = 1\n",
        "        A = A.to(dtype=torch.cfloat).to(device)\n",
        "    if T == 61: # build the A matrix using the example given in \"Trade-Offs in Decentralized Multi-Antenna Architectures: The WAX Decomposition\"\n",
        "        #Make sure that L*P = 100\n",
        "        A = torch.empty((100, 61))\n",
        "        A[ :61, :] = torch.eye(61)\n",
        "        A[61:, :39] = torch.eye(39)\n",
        "        A[61:83, 39:] = torch.eye(22)\n",
        "        A[83:, 39:56] = torch.eye(17)\n",
        "        A[83:88, 56:] = torch.eye(5)\n",
        "        A[88:93, 56:] = torch.eye(5)\n",
        "        A[93:98, 56:] = torch.eye(5)\n",
        "        A[98:, 56:58] = torch.eye(2)\n",
        "        A[98:, 58:60] = torch.eye(2)\n",
        "        A = A.to(dtype=torch.cfloat).to(device)\n",
        "    Ik = torch.eye(T, dtype=torch.cfloat).to(device)\n",
        "    Im = torch.eye(M, dtype=torch.cfloat).to(device)\n",
        "    D = torch.zeros((M, L * P)).to(device)\n",
        "    for p in range(P):\n",
        "        p_n, p_l = p * N, p * L\n",
        "        D[p_n:(p_n + N), p_l:(p_l + L)] = torch.ones((N, L))\n",
        "    return device, n_0, p_s, B, K, P, N, M, L, T, snr, T_opt, A, Ik, Im, D,J, epochs, W_is_block_diag, W_is_phase_only, X_is_variable, train_size, batch_size, pga_iters, optimizer_learning_rate\n",
        "\n",
        "\n",
        "def gen_data(seed = 43, train_size = 1500, batch_size = 20, valid_size = 250):\n",
        "    device = torch.device('cpu')  # generating all samples on the CPU for consistency\n",
        "    torch.manual_seed(seed)\n",
        "    H_train = torch.randn((train_size, B, M, K), dtype=torch.cfloat, device=device) # creating H_train in an i.i.d. manner\n",
        "    H_valid = torch.randn((valid_size, B, M, K), dtype=torch.cfloat, device=device) # creating H_valid in an i.i.d. manner\n",
        "    X = torch.eye(T, dtype=torch.cfloat)\n",
        "    W = torch.randn((M, L*P), dtype=torch.cfloat)\n",
        "\n",
        "    return H_train, H_valid, X, W\n",
        "\n",
        "\n",
        "def project_onto_block_diagonal(w,D):\n",
        "    \"\"\"\n",
        "    This function projects the matrix w onto the block diagonal matrix D.\n",
        "    :param w: the matrix to be projected\n",
        "    :param D: a predefined block diagonal matrix consisting of ones and zeros.\n",
        "    :return: the projection of w onto a block diagonal matrix.\n",
        "    \"\"\"\n",
        "    return w * D\n",
        "\n",
        "\n",
        "def project_onto_phases(w):\n",
        "    \"\"\"\n",
        "    This function projects the matrix w onto the phases of w. after projection, all entries of W has a unit magnitude.\n",
        "    :param w: the matrix to be projected\n",
        "    \"\"\"\n",
        "    W_phases = torch.exp(1j * torch.angle(w))\n",
        "    return W_phases\n",
        "\n",
        "\n",
        "class UnfoldedModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Building the unfolded model. in our code, this model is used as a skeleton for the Unfolded + Momentum algorithm.\n",
        "    Significantly outperforms the PGA algorithm.\n",
        "    Reaches almost the same performance as the Unfolded + Momentum algorithm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, Mu_W, Mu_X, J):\n",
        "        super().__init__()\n",
        "        self.Mu_W = Mu_W\n",
        "        self.Mu_X = Mu_X\n",
        "        self.J = J\n",
        "        self.D = D\n",
        "\n",
        "\n",
        "    def forward(self, H, W, X, A, p_s, n_0):\n",
        "        \"\"\"\n",
        "        This function describes the flow of the unfolded model. Here, the values of Mu_W vary in each iteration.\n",
        "        :param H: a matrix of size BxMxK containing the channel realizations for each frequency bin.\n",
        "        :param W: the inital value of W. usually randomized. It is important not to take a scaled identity matrix as the initial value of W, since it harms the convergence of the algorithm.\n",
        "        :param X: the inital value of X. set to be identity matrix.\n",
        "        :param A: the initial value of A. predefined in the set_genral_params function.\n",
        "        :return: the list of rates obtained in each iteration, the the final value of W and X.\n",
        "        \"\"\"\n",
        "        W_1 = W\n",
        "        X_1 = X\n",
        "        Rs = torch.zeros(self.J, dtype=torch.cfloat).to(device)\n",
        "        # project W onto constraints:\n",
        "        if W_is_phase_only:\n",
        "            W_1 = project_onto_phases(W_1)\n",
        "        if W_is_block_diag:\n",
        "            W_1 = project_onto_block_diagonal(W_1, self.D)\n",
        "        # starting the iterations:\n",
        "        for j in range(self.J):\n",
        "            dR_dW = self.calc_dR_dW(H, W_1, X_1, A, p_s, n_0)\n",
        "            W_1 = W_1 + self.Mu_W[j] * dR_dW # making a gradient step\n",
        "            if X_is_variable:\n",
        "                dR_dX = self.calc_dR_dX(H, W_1, X_1, A, p_s, n_0)\n",
        "                X_1 = X_1 + self.Mu_X[j] * dR_dX\n",
        "            if W_is_phase_only: # projecting W onto constraints\n",
        "                W_1 = project_onto_phases(W_1)\n",
        "            if W_is_block_diag:\n",
        "                W_1 = project_onto_block_diagonal(W_1,self.D)\n",
        "            Rs[j] = self.calc_R(H, W_1, X_1, A, p_s, n_0)\n",
        "        return Rs, W_1, X_1\n",
        "\n",
        "\n",
        "    def calc_R(self, H, W, X, A, p_s, n_0):\n",
        "        \"\"\"gets a channel realization H and returns the achievable rate of it. H is of dimension (B,M,K)\"\"\"\n",
        "        R = 0\n",
        "        B, M, K = H.shape\n",
        "        G = W @ A @ X\n",
        "        snr = p_s / n_0\n",
        "        for b in range(B):\n",
        "            h1 = H[b, :, :]\n",
        "            g1 = G\n",
        "            psudo_inv_g = torch.inverse(g1.transpose(0, 1).conj() @ g1)\n",
        "            proj_mat = g1 @ psudo_inv_g @ g1.transpose(0, 1).conj()\n",
        "            R += torch.log((Im + snr * proj_mat @ h1 @ h1.transpose(0, 1).conj()).det())\n",
        "        return R/B\n",
        "\n",
        "\n",
        "    def calc_max_r(self, H):\n",
        "        \"\"\"calclulates the maximal rate of the channel H, susbstituting G to be the matched filter. not used in the algorithm\"\"\"\n",
        "        R = 0\n",
        "        B, M, K = H.shape\n",
        "        G = H # the matched filter is obtained when G = H^H\n",
        "        snr = p_s / n_0\n",
        "        for b in range(B): # iterating over each frequency bin\n",
        "            h1 = H[b, :, :]\n",
        "            g1 = G[b, :, :]\n",
        "            psudo_inv_g = torch.inverse(g1.transpose(0, 1).conj() @ g1)\n",
        "            proj_mat = g1 @ psudo_inv_g @ g1.transpose(0, 1).conj()\n",
        "            R += torch.log((Im + snr * proj_mat @ h1 @ h1.transpose(0, 1).conj()).det())\n",
        "        return R/B # averaging over the frequency bins\n",
        "\n",
        "\n",
        "    def calc_dR_dW(self, H, W, X, A, p_s, n_0):\n",
        "        \"\"\"calculates the gradient of the rate with respect to W. the gradient has the same dimension as W\"\"\"\n",
        "        grad = 0\n",
        "        for b in range(B): #iterating over each frequency bin\n",
        "            h1 = H[b, :, :]\n",
        "            snr = p_s / n_0\n",
        "            X_con = X.conj()\n",
        "            W_con = W.conj()\n",
        "            H_con = h1.conj()\n",
        "            G = W @ A @ X\n",
        "            G_herm = G.transpose(0, 1).conj()\n",
        "            h1_herm = h1.transpose(0, 1).conj()\n",
        "            G_psu_inv = torch.inverse(G_herm @ G)\n",
        "            Proj_mat = G @ G_psu_inv @ G_herm\n",
        "            Z = snr * G_psu_inv @ G_herm @ h1 @ h1_herm @ G\n",
        "            dR_dZ = torch.inverse(Ik + Z)\n",
        "            grad += snr * (\n",
        "                        (A @ X) @ dR_dZ @ (2 * G_psu_inv @ G_herm) @ h1 @ h1_herm @ (Im - Proj_mat)).conj().transpose(0,                                                                                                       1)\n",
        "        return grad / B # averaging over the frequency bins\n",
        "\n",
        "\n",
        "    def calc_dR_dX(self, H, W, X, A, p_s, n_0):\n",
        "        \"\"\"calculates the gradient of the rate with respect to X. the gradient has the same dimension as X\"\"\"\n",
        "        \"\"\" not used in our code! \"\"\"\n",
        "        snr = p_s / n_0\n",
        "        X_con = X.conj()\n",
        "        W_con = W.conj()\n",
        "        H_con = H.conj()\n",
        "        G = W @ A @ X\n",
        "        G_herm = G.transpose(0, 1).conj()\n",
        "        H_herm = H.transpose(1, 2).conj()\n",
        "        G_psu_inv = torch.inverse(G_herm @ G)\n",
        "        Proj_mat = G @ G_psu_inv @ G_herm\n",
        "        Z = snr * G_psu_inv @ G_herm @ H @ H_herm @ G\n",
        "        dR_dZ = torch.inverse(Ik + Z)\n",
        "        return snr * (dR_dZ @ (2 * G_psu_inv @ G_herm) @ H @ H_herm @ (Im - Proj_mat) @ W @ A).conj().transpose(1, 2)\n",
        "\n",
        "\n",
        "    def batch_rate(self, batch, W, X, A, p_s, n_0):\n",
        "        \"\"\" Used in order to calculate the average rate of a batch. Calling the forward functon for each sample in the batch\"\"\"\n",
        "        batch_rate = 0\n",
        "        for sample_num in range(batch.shape[0]):\n",
        "            sample = batch[sample_num, :, :, :]\n",
        "            Rs, W_1, X_1 = self.forward(sample, W, X, A, p_s, n_0)\n",
        "            batch_rate += Rs\n",
        "        return batch_rate / (batch.shape[0]), W_1, X_1\n",
        "\n",
        "\n",
        "class PGAModel():\n",
        "    def __init__(self, Mu, pga_iters):\n",
        "        \"\"\"\n",
        "        The classical implementation of the PGA algorithm, a first order method the approch a maximum in a convex problem.\n",
        "        :param Mu: The step size of the algorithm, a hyperparameter\n",
        "        :param pga_iters: The duration of the algorithm, a hyperparameter\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.Mu = Mu\n",
        "        self.pga_iters = pga_iters\n",
        "        self.D = D\n",
        "\n",
        "\n",
        "    def forward(self, H, W, X, A, p_s, n_0):\n",
        "        \"\"\" based on the forward of the unfolded algorithm, but the value of Mu is fixed \"\"\"\n",
        "        W_1 = W.detach()\n",
        "        X_1 = X.detach()\n",
        "        Rs = torch.zeros(self.pga_iters, dtype=torch.cfloat).to(device)\n",
        "        if W_is_phase_only:\n",
        "            W_1 = project_onto_phases(W_1)\n",
        "        if W_is_block_diag:\n",
        "            W_1 = project_onto_block_diagonal(W_1, self.D)\n",
        "        for j in range(self.pga_iters):\n",
        "            dR_dW = self.calc_dR_dW(H, W_1, X_1, A, p_s, n_0)\n",
        "            W_1 = W_1 + self.Mu * dR_dW\n",
        "            if X_is_variable:\n",
        "                dR_dX = self.calc_dR_dX(H, W_1, X_1, A, p_s, n_0)\n",
        "                X_1 = X_1 + self.Mu * dR_dX\n",
        "            if W_is_phase_only:\n",
        "                W_1 = project_onto_phases(W_1)\n",
        "            if W_is_block_diag:\n",
        "                W_1 = project_onto_block_diagonal(W_1, self.D)\n",
        "            Rs[j] = self.calc_R(H, W_1, X_1, A, p_s, n_0)\n",
        "        return Rs, W_1, X_1\n",
        "\n",
        "\n",
        "    def calc_R(self, H, W, X, A, p_s, n_0):\n",
        "        \"\"\"gets a channel realization and returns the achievable rate of it. H is of dimension (B,M,K)\"\"\"\n",
        "        R = 0\n",
        "        B, M, K = H.shape\n",
        "        G = W @ A @ X\n",
        "        snr = p_s / n_0\n",
        "        for b in range(B):\n",
        "            h1 = H[b, :, :]\n",
        "            g1 = G\n",
        "            psudo_inv_g = torch.inverse(g1.transpose(0, 1).conj() @ g1)\n",
        "            proj_mat = g1 @ psudo_inv_g @ g1.transpose(0, 1).conj()\n",
        "            I_plus_Z = Im + snr * proj_mat @ h1 @ h1.transpose(0, 1).conj()\n",
        "            det_I_plus_Z = torch.det(I_plus_Z.to(torch.complex128))\n",
        "            R += torch.log(det_I_plus_Z)\n",
        "        return R/B\n",
        "\n",
        "\n",
        "    def calc_max_r(self, H):\n",
        "        \"\"\"calclulates the maximal rate of the channel H, susbstituting G to be the matched filter. not used in the algorithm\"\"\"\n",
        "        R = 0\n",
        "        B, M, K = H.shape\n",
        "        G = H\n",
        "        snr = p_s / n_0\n",
        "        for b in range(B):\n",
        "            h1 = H[b, :, :]\n",
        "            g1 = G[b, :, :]\n",
        "            psudo_inv_g = torch.inverse(g1.transpose(0, 1).conj() @ g1)\n",
        "            proj_mat = g1 @ psudo_inv_g @ g1.transpose(0, 1).conj()\n",
        "            I_plus_Z = Im + snr * proj_mat @ h1 @ h1.transpose(0, 1).conj()\n",
        "            det_I_plus_Z = torch.det(I_plus_Z.to(torch.complex128))\n",
        "            R += torch.log(det_I_plus_Z)\n",
        "        return R/B\n",
        "\n",
        "\n",
        "    def calc_dR_dW(self, H, W, X, A, p_s, n_0):\n",
        "        \"\"\"calculates the gradient of the rate with respect to W. the gradient has the same dimension as W\"\"\"\n",
        "        grad= 0\n",
        "        for b in range(B):\n",
        "            h1 = H[b, :, :]\n",
        "            snr = p_s / n_0\n",
        "            X_con = X.conj()\n",
        "            W_con = W.conj()\n",
        "            H_con = h1.conj()\n",
        "            G = W @ A @ X\n",
        "            G_herm = G.transpose(0, 1).conj()\n",
        "            h1_herm = h1.transpose(0, 1).conj()\n",
        "            G_psu_inv = torch.inverse(G_herm @ G)\n",
        "            Proj_mat = G @ G_psu_inv @ G_herm\n",
        "            Z = snr * G_psu_inv @ G_herm @ h1 @ h1_herm @ G\n",
        "            dR_dZ = torch.inverse(Ik + Z)\n",
        "            grad += snr * ((A @ X) @ dR_dZ @ (2 * G_psu_inv @ G_herm) @ h1 @ h1_herm @ (Im - Proj_mat)).conj().transpose(0, 1)\n",
        "        return grad / B\n",
        "\n",
        "\n",
        "    def calc_dR_dX(self, H, W, X, A, p_s, n_0):\n",
        "        \"\"\"calculates the gradient of the rate with respect to X. the gradient has the same dimension as X\"\"\"\n",
        "        \"\"\" not used in our code! \"\"\"\n",
        "        snr = p_s / n_0\n",
        "        X_con = X.conj()\n",
        "        W_con = W.conj()\n",
        "        H_con = H.conj()\n",
        "        G = W @ A @ X\n",
        "        G_herm = G.transpose(0,1).conj()\n",
        "        H_herm = H.transpose(1, 2).conj()\n",
        "        G_psu_inv = torch.inverse(G_herm @ G)\n",
        "        Proj_mat = G @ G_psu_inv @ G_herm\n",
        "        Z = snr * G_psu_inv @ G_herm @ H @ H_herm @ G\n",
        "        dR_dZ = torch.inverse(Ik + Z)\n",
        "        return snr * (dR_dZ @ (2 * G_psu_inv @ G_herm) @ H @ H_herm @ (Im - Proj_mat) @ W @ A).conj().transpose(1, 2)\n",
        "\n",
        "\n",
        "    def batch_rate(self, batch, W, X, A, p_s, n_0):\n",
        "        \"\"\" Used in order to calculate the average rate of a batch. Calling the forward functon for each sample in the batch\"\"\"\n",
        "        batch_rate = 0\n",
        "        for sample_num in range(batch.shape[0]):\n",
        "            sample = batch[sample_num, :, :, :]\n",
        "            Rs, W_1, X_1 = self.forward(sample, W, X, A, p_s, n_0)\n",
        "            batch_rate += Rs\n",
        "        return batch_rate / (batch.shape[0]), W_1, X_1\n",
        "\n",
        "\n",
        "class PGAModelMomentum():\n",
        "    def __init__(self, Mu,beta, pga_iters):\n",
        "        \"\"\" Works exactly the same like the PGAModel, but has a value of beta, which is the momentum parameter. \"\"\"\n",
        "        super().__init__()\n",
        "        self.Mu = Mu  # step size parameter.\n",
        "        self.pga_iters = pga_iters  # number of iterations of the PGA algorithm.\n",
        "        self.D = D  # projction matrix\n",
        "        self.beta = beta  # momentum parameter.\n",
        "\n",
        "\n",
        "    def batch_rate(self, batch, W, X, A, p_s, n_0):\n",
        "        \"\"\" Used in order to calculate the average rate of a batch. Calling the forward functon for each sample in the batch\"\"\"\n",
        "        batch_rate = 0\n",
        "        for sample_num in range(batch.shape[0]):\n",
        "            sample = batch[sample_num, :, :, :]\n",
        "            Rs, W_1, X_1 = self.forward(sample, W, X, A, p_s, n_0)\n",
        "            batch_rate += Rs\n",
        "        return batch_rate / (batch.shape[0]), W_1, X_1\n",
        "\n",
        "\n",
        "    def forward(self, H, W, X, A, p_s, n_0):\n",
        "        \"\"\"\n",
        "        This function describes the flow of the PGA + Momentum model.\n",
        "        Mu and beta are fixed scalars.\n",
        "       :param H: a matrix of size BxMxK containing the channel realizations for each frequency bin.\n",
        "       :param W: the inital value of W. usually randomized. It is important not to take ones or zeros matrix as the initial value of W, since it harms the convergence of the algorithm.\n",
        "       :param X: the inital value of X. set to be identity matrix.\n",
        "       :param A: the initial value of A. predefined in the set_genral_params function.\n",
        "       :return: the list of rates obtained in each iteration, the the final value of W and X.\n",
        "       \"\"\"\n",
        "        W_1 = W\n",
        "        X_1 = X\n",
        "        if W_is_phase_only:\n",
        "            W_1 = project_onto_phases(W_1)\n",
        "        if W_is_block_diag:\n",
        "            W_1 = project_onto_block_diagonal(W_1, self.D)\n",
        "        W_0 = torch.zeros_like(W_1) # initizlized as the zero matrix.\n",
        "        Rs = torch.zeros(self.pga_iters, dtype=torch.cfloat).to(device)\n",
        "        for j in range(self.pga_iters):\n",
        "            dR_dW = self.calc_dR_dW(H, W_1, X_1, A, p_s, n_0)\n",
        "            W_2 = W_1 + self.Mu * dR_dW + self.beta * (W_1 - W_0)\n",
        "            if W_is_phase_only:\n",
        "                W_2 = project_onto_phases(W_2)\n",
        "            if W_is_block_diag:\n",
        "                W_2 = project_onto_block_diagonal(W_2, self.D)\n",
        "            Rs[j] = self.calc_R(H, W_2, X_1, A, p_s, n_0)\n",
        "            # updating the matrices:\n",
        "            W_0 = W_1\n",
        "            W_1 = W_2\n",
        "            X_0 = X_1\n",
        "        return Rs, W_2, X_1\n",
        "\n",
        "\n",
        "    def calc_R(self, H, W, X, A, p_s, n_0):\n",
        "        \"\"\"gets a channel realization and returns the achievable rate of it. h is of dimension (B,M,K)\"\"\"\n",
        "        R = 0\n",
        "        B, M, K = H.shape\n",
        "        G = W @ A @ X\n",
        "        snr = p_s / n_0\n",
        "        for b in range(B):\n",
        "            h1 = H[b, :, :]\n",
        "            g1 = G\n",
        "            psudo_inv_g = torch.inverse(g1.transpose(0, 1).conj() @ g1)\n",
        "            proj_mat = g1 @ psudo_inv_g @ g1.transpose(0, 1).conj()\n",
        "            I_plus_Z = Im + snr * proj_mat @ h1 @ h1.transpose(0, 1).conj()\n",
        "            det_I_plus_Z = torch.det(I_plus_Z.to(torch.complex128))\n",
        "            R += torch.log(det_I_plus_Z)\n",
        "        return R/B\n",
        "\n",
        "\n",
        "    def calc_max_r(self, H):\n",
        "        \"\"\"calclulates the maximal rate of the channel H, susbstituting G to be the matched filter. not used in the algorithm\"\"\"\n",
        "        R = 0\n",
        "        B, M, K = H.shape\n",
        "        G = H\n",
        "        snr = p_s / n_0\n",
        "        for b in range(B):\n",
        "            h1 = H[b, :, :]\n",
        "            g1 = G[b, :, :]\n",
        "            psudo_inv_g = torch.inverse(g1.transpose(0, 1).conj() @ g1)\n",
        "            proj_mat = g1 @ psudo_inv_g @ g1.transpose(0, 1).conj()\n",
        "            I_plus_Z = Im + snr * proj_mat @ h1 @ h1.transpose(0, 1).conj()\n",
        "            det_I_plus_Z = torch.det(I_plus_Z.to(torch.complex128))\n",
        "            R += torch.log(det_I_plus_Z)\n",
        "        return R/B\n",
        "\n",
        "\n",
        "    def calc_dR_dW(self, H, W, X, A, p_s, n_0):\n",
        "        \"\"\"calculates the gradient of the rate with respect to W. the gradient has the same dimension as W\"\"\"\n",
        "        grad = 0\n",
        "        for b in range(B):\n",
        "            h1 = H[b, :, :]\n",
        "            snr = p_s / n_0\n",
        "            X_con = X.conj()\n",
        "            W_con = W.conj()\n",
        "            H_con = h1.conj()\n",
        "            G = W @ A @ X\n",
        "            G_herm = G.transpose(0, 1).conj()\n",
        "            h1_herm = h1.transpose(0, 1).conj()\n",
        "            G_psu_inv = torch.inverse(G_herm @ G)\n",
        "            Proj_mat = G @ G_psu_inv @ G_herm\n",
        "            Z = snr * G_psu_inv @ G_herm @ h1 @ h1_herm @ G\n",
        "            dR_dZ = torch.inverse(Ik + Z)\n",
        "            grad += snr * ((A @ X) @ dR_dZ @ (2 * G_psu_inv @ G_herm) @ h1 @ h1_herm @ (Im - Proj_mat)).conj().transpose(0,1)\n",
        "        return grad / B\n",
        "\n",
        "\n",
        "    def calc_dR_dX(self, H, W, X, A, p_s, n_0):\n",
        "        \"\"\"calculates the gradient of the rate with respect to X. the gradient has the same dimension as X\"\"\"\n",
        "        \"\"\" not used in our code! \"\"\"\n",
        "        snr = p_s / n_0\n",
        "        X_con = X.conj()\n",
        "        W_con = W.conj()\n",
        "        H_con = H.conj()\n",
        "        # Im = torch.eye(M)\n",
        "        G = W @ A @ X\n",
        "        G_herm = G.transpose(0, 1).conj()\n",
        "        H_herm = H.transpose(1, 2).conj()\n",
        "        G_psu_inv = torch.inverse(G_herm @ G)\n",
        "        Proj_mat = G @ G_psu_inv @ G_herm\n",
        "        Z = snr * G_psu_inv @ G_herm @ H @ H_herm @ G\n",
        "        dR_dZ = torch.inverse(Ik + Z)\n",
        "        return snr * (dR_dZ @ (2 * G_psu_inv @ G_herm) @ H @ H_herm @ (Im - Proj_mat) @ W @ A).conj().transpose(1, 2)\n",
        "\n",
        "\n",
        "class MomentumModel(nn.Module):\n",
        "    \"\"\" The model that outperforms all others. This is the innovative part in our work \"\"\"\n",
        "    def __init__(self, Mu_W, Mu_X,betas, J):\n",
        "        \"\"\" Works exactly the same like the Unfolded Model, but has a value of beta, which is the momentum parameter. \"\"\"\n",
        "        super().__init__()\n",
        "        self.Mu_W = Mu_W  # the step size for each entry of W. varies within every iteration of the algorhtim. Has the shape of W.\n",
        "        self.Mu_X = Mu_X\n",
        "        self.J = J\n",
        "        self.D = D\n",
        "        self.betas = betas  # the momentum parameter for every entry of W. varies within every iteration of the algorhtim. Has the shape of W.\n",
        "\n",
        "\n",
        "    def batch_rate(self, batch, W, X, A, p_s, n_0):\n",
        "        \"\"\" Used in order to calculate the average rate of a batch. Calling the forward functon for each sample in the batch \"\"\"\n",
        "        batch_rate = 0\n",
        "        for sample_num in range(batch.shape[0]):\n",
        "            sample = batch[sample_num, :, :, :]\n",
        "            Rs, W_1, X_1 = self.forward(sample, W, X, A, p_s, n_0)\n",
        "            batch_rate += Rs\n",
        "        return batch_rate / (batch.shape[0]), W_1, X_1\n",
        "\n",
        "\n",
        "    def forward(self, H, W, X, A, p_s, n_0):\n",
        "        \"\"\"\n",
        "        This function describes the flow of the Unfolded PGA + Momentum model.\n",
        "        Here, the values of Mu_W and beta vary in each iteration.\n",
        "        Mu_W and beta are matrices with the same size as W.\n",
        "       :param H: a matrix of size BxMxK containing the channel realizations for each frequency bin.\n",
        "       :param W: the inital value of W. usually randomized. It is important not to take ones or zeros matrix as the initial value of W, since it harms the convergence of the algorithm.\n",
        "       :param X: the inital value of X. set to be identity matrix.\n",
        "       :param A: the initial value of A. predefined in the set_genral_params function.\n",
        "       :return: the list of rates obtained in each iteration, the the final value of W and X.\n",
        "       \"\"\"\n",
        "        W_1 = W\n",
        "        X_1 = X\n",
        "        # projecting W on the constraints:\n",
        "        if W_is_phase_only:\n",
        "            W_1 = project_onto_phases(W_1)\n",
        "        if W_is_block_diag:\n",
        "            W_1 = project_onto_block_diagonal(W_1, self.D)\n",
        "        W_0 = torch.zeros_like(W_1)\n",
        "        Rs = torch.zeros(self.J, dtype=torch.cfloat).to(device)\n",
        "        # Starting the forward path:\n",
        "        for j in range(self.J):\n",
        "            dR_dW = self.calc_dR_dW(H, W_1, X_1, A, p_s, n_0) # calculating the gradient of the rate with respect to W\n",
        "            W_2 = W_1 + self.Mu_W[j] * dR_dW + self.betas[j] * (W_1 - W_0) # updating W\n",
        "            # projecting W on the constraints:\n",
        "            if W_is_phase_only:\n",
        "                W_2 = project_onto_phases(W_2)\n",
        "            if W_is_block_diag:\n",
        "                W_2 = project_onto_block_diagonal(W_2,self.D)\n",
        "            Rs[j] = self.calc_R(H, W_2, X_1, A, p_s, n_0)  # documenting the rate\n",
        "            # updating the matrices:\n",
        "            W_0 = W_1\n",
        "            W_1 = W_2\n",
        "        return Rs, W_2, X_1\n",
        "\n",
        "\n",
        "    def calc_R(self, H, W, X, A, p_s, n_0):\n",
        "        \"\"\"gets a channel realization and returns the achievable rate of it. h is of dimension (B,M,K)\"\"\"\n",
        "        R = 0\n",
        "        B, M, K = H.shape\n",
        "        G = W @ A @ X\n",
        "        snr = p_s / n_0\n",
        "        for b in range(B):\n",
        "            h1 = H[b, :, :]\n",
        "            g1 = G\n",
        "            psudo_inv_g = torch.inverse(g1.transpose(0, 1).conj() @ g1)\n",
        "            proj_mat = g1 @ psudo_inv_g @ g1.transpose(0, 1).conj()\n",
        "            R += torch.log((Im + snr * proj_mat @ h1 @ h1.transpose(0, 1).conj()).det())\n",
        "        return R/B\n",
        "\n",
        "\n",
        "    def calc_max_r(self, H):\n",
        "        \"\"\"calclulates the maximal rate of the channel H, susbstituting G to be the matched filter. not used in the algorithm\"\"\"\n",
        "        R = 0\n",
        "        B, M, K = H.shape\n",
        "        G = H\n",
        "        snr = p_s / n_0\n",
        "        for b in range(B):\n",
        "            h1 = H[b, :, :]\n",
        "            g1 = G[b, :, :]\n",
        "            psudo_inv_g = torch.inverse(g1.transpose(0, 1).conj() @ g1)\n",
        "            proj_mat = g1 @ psudo_inv_g @ g1.transpose(0, 1).conj()\n",
        "            R += torch.log((Im + snr * proj_mat @ h1 @ h1.transpose(0, 1).conj()).det())\n",
        "        return R/B\n",
        "\n",
        "\n",
        "    def calc_dR_dW(self, H, W, X, A, p_s, n_0):\n",
        "        \"\"\"calculates the gradient of the rate with respect to W. the gradient has the same dimension as W\"\"\"\n",
        "        grad = 0\n",
        "        for b in range(B):\n",
        "            h1 = H[b, :, :]\n",
        "            snr = p_s / n_0\n",
        "            X_con = X.conj()\n",
        "            W_con = W.conj()\n",
        "            H_con = h1.conj()\n",
        "            G = W @ A @ X\n",
        "            G_herm = G.transpose(0, 1).conj()\n",
        "            h1_herm = h1.transpose(0, 1).conj()\n",
        "            G_psu_inv = torch.inverse(G_herm @ G)\n",
        "            Proj_mat = G @ G_psu_inv @ G_herm\n",
        "            Z = snr * G_psu_inv @ G_herm @ h1 @ h1_herm @ G\n",
        "            dR_dZ = torch.inverse(Ik + Z)\n",
        "            grad += snr * ((A @ X) @ dR_dZ @ (2 * G_psu_inv @ G_herm) @ h1 @ h1_herm @ (Im - Proj_mat)).conj().transpose(0,1)\n",
        "        return grad / B\n",
        "\n",
        "\n",
        "    def calc_dR_dX(self, H, W, X, A, p_s, n_0):\n",
        "        \"\"\"calculates the gradient of the rate with respect to X. the gradient has the same dimension as X\"\"\"\n",
        "        \"\"\" not used in our code! \"\"\"\n",
        "        snr = p_s / n_0\n",
        "        X_con = X.conj()\n",
        "        W_con = W.conj()\n",
        "        H_con = H.conj()\n",
        "        G = W @ A @ X\n",
        "        G_herm = G.transpose(1, 2).conj()\n",
        "        H_herm = H.transpose(1, 2).conj()\n",
        "        G_psu_inv = torch.inverse(G_herm @ G)\n",
        "        Proj_mat = G @ G_psu_inv @ G_herm\n",
        "        Z = snr * G_psu_inv @ G_herm @ H @ H_herm @ G\n",
        "        dR_dZ = torch.inverse(Ik + Z)\n",
        "        return snr * (dR_dZ @ (2 * G_psu_inv @ G_herm) @ H @ H_herm @ (Im - Proj_mat) @ W @ A).conj().transpose(1, 2)\n",
        "\n",
        "\n",
        "def train_unfolding(H_train, H_valid, X, W, A, p_s, n_0, J, Mu_X_init, Mu_W_init,\n",
        "          optimizer_learning_rate, epochs, train_size, batch_size):\n",
        "    \"\"\" Trains the unfolded model - not the momentum model.\"\"\"\n",
        "    \"\"\" Returns the trained model and the training and validation rates. Using ADAM optimizer and maximizing the weighted sum rate function of each iteration. \"\"\"\n",
        "    # Setting the variables:\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    H_train, H_valid, X, W, A = H_train.to(device), H_valid.to(device), X.to(device), W.to(device), A.to(device)\n",
        "    Mu_W = torch.full((J, M, L * P), Mu_W_init, requires_grad=True, device = device)\n",
        "    Mu_X = torch.full((J, T, T), Mu_X_init, requires_grad=X_is_variable, device=device)\n",
        "    # crating the model and setting the optimizer\n",
        "    unfolded_model = UnfoldedModel(Mu_W, Mu_X, J).to(device)\n",
        "    optimizer = torch.optim.Adam([Mu_W], lr=optimizer_learning_rate)\n",
        "    # projecting W on the constraints\n",
        "    if W_is_phase_only:\n",
        "        W = project_onto_phases(W)\n",
        "    if W_is_block_diag:\n",
        "        W = project_onto_block_diagonal(W,D)\n",
        "    train_rates = [0]*(epochs*int(np.ceil(train_size/batch_size)))\n",
        "    validation_rates = [0]*(epochs)\n",
        "    i = 0\n",
        "    # Start training:\n",
        "    print(\"Unfolded starts training\")\n",
        "    for epoch in range(epochs):\n",
        "        print(\"epoch: \", epoch + 1, \" of \", epochs)\n",
        "        initial_time = time.time()\n",
        "        # Shuffling the training data:\n",
        "        H_train = H_train[torch.randperm(H_train.shape[0])]\n",
        "        for iter in range(0, train_size, batch_size):\n",
        "            # dividing the data into batches:\n",
        "            H = H_train[iter:iter + batch_size, :, :, :]\n",
        "            Rs, W_1, X_1 = unfolded_model.batch_rate(H, W=W, X=X, A=A, p_s=p_s, n_0=n_0)  # Rs is the list of the calculated rates in each iteration.\n",
        "            loss = -  torch.sum(Rs * torch.log(torch.arange(2, J + 2, device=device)))  # Negating the weighted loss since we want to maximize the rate.\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_rates[i] = Rs[-1]\n",
        "            i = i+1\n",
        "        # At the end of each epoch we calculate the validation rate:\n",
        "        validation_rate, _, _ = unfolded_model.batch_rate(H_valid, W=W, X=X, A=A,p_s=p_s,n_0=n_0)\n",
        "        validation_rates[epoch] = validation_rate\n",
        "        print(\"epoch time: \", round(time.time() - initial_time, 2), \" seconds\")\n",
        "    return unfolded_model, train_rates, validation_rates\n",
        "\n",
        "\n",
        "def run_PGA_Momentum(PGA_Mu_init, beta, pga_iters, H_valid, W, X, A, p_s, n_0):\n",
        "    \"\"\" Running the PGA + Momentum, with fixes Mu and Beta. Returns the list of calculated sum-rate for each iteration. \"\"\"\n",
        "    \"\"\" Used as a benchmark comparing the performance of the unfolded algorithm with the classical one. \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # projecting W on the constraints\n",
        "    if W_is_phase_only:\n",
        "        W = project_onto_phases(W)\n",
        "    if W_is_block_diag:\n",
        "        W = project_onto_block_diagonal(W, D)\n",
        "    # moving the variables to \"device\":\n",
        "    H_valid,W,X,A = H_valid.to(device), W.to(device),X.to(device),A.to(device)\n",
        "    # setting the model and running the PGA:\n",
        "    pga_model_Momentum = PGAModelMomentum(Mu=PGA_Mu_init,beta=beta, pga_iters=pga_iters)\n",
        "    Rs_pga, W_2, X_2 = pga_model_Momentum.batch_rate(H_valid, W=W, X=X,A=A, p_s=p_s, n_0=n_0)\n",
        "    return Rs_pga, W_2, X_2\n",
        "\n",
        "\n",
        "def run_PGA(PGA_Mu_init, pga_iters, H_valid, W, X, A, p_s, n_0):\n",
        "    \"\"\" Running the classical PGA , with fixes Mu and Beta. Returns the list of calculated sum-rate for each iteration. \"\"\"\n",
        "    \"\"\" Not used in our code. \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    if W_is_phase_only:\n",
        "        W = project_onto_phases(W)\n",
        "    if W_is_block_diag:\n",
        "        W = project_onto_block_diagonal(W, D)\n",
        "    H_valid,W,X,A = H_valid.to(device), W.to(device),X.to(device),A.to(device)\n",
        "    pga_model = PGAModel(Mu=PGA_Mu_init, pga_iters=pga_iters)\n",
        "    Rs_pga, W_2, X_2 = pga_model.batch_rate(H_valid, W=W, X=X,A=A, p_s=p_s, n_0=n_0)\n",
        "    return Rs_pga, W_2, X_2\n",
        "\n",
        "def line_search(H_valid, X, W, A, p_s, n_0, grid, line_search_iters):\n",
        "    \"\"\" Iterates over every sample in the validation set, and finds the best step size for each sample. \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # projecting W on the constraints:\n",
        "    if W_is_phase_only:\n",
        "        W = project_onto_phases(W)\n",
        "    if W_is_block_diag:\n",
        "        W = project_onto_block_diagonal(W, D)\n",
        "    H_valid, W, X, A = H_valid.to(device), W.to(device), X.to(device), A.to(device)\n",
        "    W_1 = W.detach()\n",
        "    X_1 = X.detach()\n",
        "    # Initializing the PGA + M object:\n",
        "    pga_mom_model = PGAModelMomentum(Mu=0,beta=0.9, pga_iters=line_search_iters)\n",
        "    best_steps = torch.zeros((H_valid.shape[0], line_search_iters), device=device)\n",
        "    best_Rs = torch.zeros((H_valid.shape[0], line_search_iters), device=device)\n",
        "    # Takes a single sample:\n",
        "    for sample_num in range(H_valid.shape[0]):\n",
        "        W_1 = W.detach()\n",
        "        X_1 = X.detach()\n",
        "        sample = H_valid[sample_num, :, :, :]\n",
        "        # Start moving over all the iterations in \"line_search_iters\" (Usually 50).\n",
        "        for iteration in range(line_search_iters):\n",
        "            if W_is_phase_only:\n",
        "                W_1 = project_onto_phases(W_1)\n",
        "            if W_is_block_diag:\n",
        "                W_1 = project_onto_block_diagonal(W_1, D)\n",
        "            # Calculates the gradient:\n",
        "            grad = pga_mom_model.calc_dR_dW(sample, W_1, X_1, A, p_s, n_0)\n",
        "            best_step = 0\n",
        "            best_R = -1\n",
        "            # Looking for the best step size with the calculated gradient:\n",
        "            for step in grid:\n",
        "                W_new = W_1 + step * grad  # Updating W\n",
        "                if W_is_phase_only:\n",
        "                    W_new = project_onto_phases(W_new)\n",
        "                if W_is_block_diag:\n",
        "                    W_new = project_onto_block_diagonal(W_new, D)\n",
        "                # Calculating the optional R.\n",
        "                R = pga_mom_model.calc_R(sample, W_new, X, A, p_s, n_0)\n",
        "                if abs(R) > abs(best_R): # If the calculated R is better than the best R so far, save the step size.\n",
        "                    best_R = R\n",
        "                    best_step = step\n",
        "            # Before the next iteration, update W with the best step chosen.\n",
        "            W_1 += best_step * grad\n",
        "            if W_is_phase_only:\n",
        "                W_1 = project_onto_phases(W_1)\n",
        "            if W_is_block_diag:\n",
        "                W_1 = project_onto_block_diagonal(W_1,D)\n",
        "            best_steps[sample_num,iteration] = best_step # saving the best step and the best R.\n",
        "            best_Rs[sample_num,iteration] = best_R\n",
        "    average_best_Rs = torch.mean(best_Rs,axis=0)  # Calculate the mean R\n",
        "    average_best_steps = torch.mean(best_steps,axis=0)  # Calculate the mean best step size, for comparison reasons.\n",
        "    return average_best_Rs, average_best_steps\n",
        "\n",
        "\n",
        "def train_momentum(H_train, H_valid, X, W, A,p_s,n_0, J, Mu_X_init, Mu_W_init,\n",
        "          optimizer_learning_rate, epochs, train_size, batch_size, betas_init):\n",
        "    \"\"\" Trains the unfolded model + Momentum. Returns the trained model and the training and validation rates.\n",
        "     Using ADAM optimizer and maximizing the weighted sum rate function of each iteration.\"\"\"\n",
        "    # setting the device to be GPU to accalerate calculations.\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    if W_is_phase_only:\n",
        "        W = project_onto_phases(W)\n",
        "    if W_is_block_diag:\n",
        "        W = project_onto_block_diagonal(W,D)\n",
        "    # saving variables in device:\n",
        "    H_train, H_valid, X, W, A = H_train.to(device), H_valid.to(device), X.to(device), W.to(device), A.to(device)\n",
        "    Mu_W = torch.full((J, M, L * P), Mu_W_init, requires_grad=True, device=device)\n",
        "    Mu_X = torch.full((J, T, T), Mu_X_init, requires_grad=X_is_variable, device=device)\n",
        "    betas = torch.full((J, M, L * P), betas_init, requires_grad=True, device = device)\n",
        "    momentum_model = MomentumModel(Mu_W, Mu_X,betas, J).to(device)\n",
        "    optimizer = torch.optim.Adam([Mu_W, betas], lr=optimizer_learning_rate)\n",
        "    if W_is_phase_only:\n",
        "        W = project_onto_phases(W)\n",
        "    if W_is_block_diag:\n",
        "        W = project_onto_block_diagonal(W,D)\n",
        "    train_rates = [0]*(epochs*int(np.ceil(train_size/batch_size)))\n",
        "    validation_rates = [0]*(epochs)\n",
        "    i = 0\n",
        "    # Starting training procedure:\n",
        "    print(\"Momentum starts training\")\n",
        "    for epoch in range(epochs):\n",
        "        print(\"epoch: \", epoch + 1, \" of \", epochs)\n",
        "        initial_time = time.time()\n",
        "        H_train = H_train[torch.randperm(H_train.shape[0])]  # Shuffles the training data.\n",
        "        for iter in range(0, train_size, batch_size):\n",
        "            H = H_train[iter:iter + batch_size, :, :, :]  # dividing the data to batches.\n",
        "            Rs, W_1, X_1 = momentum_model.batch_rate(H, W=W, X=X, A=A, p_s=p_s, n_0=n_0)  # Rs is the list of the calculated rates in each iteration.\n",
        "            loss = -  torch.sum(Rs * torch.log(torch.arange(2, J + 2, device=device)))  # Negating the weighted loss since we want to maximize the rate.\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()  # Making the backward step.\n",
        "            optimizer.step()\n",
        "            train_rates[i] = Rs[-1]\n",
        "            i = i+1\n",
        "        # At the end of each epoch we calculate the validation rate:\n",
        "        validation_rate, _, _ = momentum_model.batch_rate(H_valid, W=W, X=X, A=A,p_s=p_s,n_0=n_0)\n",
        "        validation_rates[epoch] = validation_rate\n",
        "        print(\"epoch time: \", round(time.time() - initial_time, 2), \" seconds\")\n",
        "    return momentum_model, train_rates, validation_rates\n",
        "\n",
        "\n",
        "def constant_search(H_valid, W, X, A, p_s, n_0,constant_search_iters, grid):\n",
        "    \"\"\" A function that finds the best Mu from a given grid, for a classical PGA algorithm without momentum. Not used in our code. \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    H_valid, W, X, A = H_valid.to(device), W.to(device), X.to(device), A.to(device)\n",
        "    W_1 = W.detach()\n",
        "    X_1 = X.detach()\n",
        "    if W_is_phase_only:\n",
        "        W_1 = project_onto_phases(W_1)\n",
        "    if W_is_block_diag:\n",
        "        W_1 = project_onto_block_diagonal(W_1, D)\n",
        "    best_R = 0\n",
        "    best_step = 0\n",
        "    for step in grid:\n",
        "        pga_model = PGAModel(Mu=step, pga_iters=constant_search_iters)\n",
        "        R,_,_ = pga_model.batch_rate(H_valid, W_1, X_1, A, p_s, n_0)\n",
        "        if abs(R[-1]) > abs(best_R):\n",
        "            best_R = R[-1]\n",
        "            best_step = step\n",
        "    return best_step, best_R\n",
        "\n",
        "\n",
        "def beta_search(H_valid, W, X, A, p_s, n_0,constant_search_iters, grid, best_Mu):\n",
        "    \"\"\" A function that finds the best beta from a given grid, for a PGA + M , with a fixed Mu. Not used in our code. \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    H_valid, W, X, A = H_valid.to(device), W.to(device), X.to(device), A.to(device)\n",
        "    W_1 = W.detach()\n",
        "    X_1 = X.detach()\n",
        "    if W_is_phase_only:\n",
        "        W_1 = project_onto_phases(W_1)\n",
        "    if W_is_block_diag:\n",
        "        W_1 = project_onto_block_diagonal(W_1, D)\n",
        "    best_R = 0\n",
        "    best_step = 0\n",
        "    for step in grid:\n",
        "        pga_model_mom = PGAModelMomentum(Mu=best_Mu,beta=step, pga_iters=constant_search_iters)\n",
        "        R,_,_ = pga_model_mom.batch_rate(H_valid, W_1, X_1, A, p_s, n_0)\n",
        "        if abs(R[-1]) > abs(best_R):\n",
        "            best_R = R[-1]\n",
        "            best_step = step\n",
        "    return best_step, best_R\n",
        "\n",
        "\n",
        "def mu_and_beta_search(H_valid, W, X, A, p_s, n_0,constant_search_iters, grid_mu, grid_beta):\n",
        "    \"\"\" A function that finds the best Mu and beta combination from some given grids, for a classical PGA + M.\n",
        "     Used in order to find the optimal values of PGA + M as benchmarks.\n",
        "     The unfolded model is initiated with the values found here. \"\"\"\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    H_valid, W, X, A = H_valid.to(device), W.to(device), X.to(device), A.to(device)\n",
        "    W_1 = W.detach()\n",
        "    X_1 = X.detach()\n",
        "    if W_is_phase_only:\n",
        "        W_1 = project_onto_phases(W_1)\n",
        "    if W_is_block_diag:\n",
        "        W_1 = project_onto_block_diagonal(W_1, D)\n",
        "    best_R = 0\n",
        "    best_mu = 0\n",
        "    best_beta = 0\n",
        "    pga_model_mom = PGAModelMomentum(Mu=0, beta=0, pga_iters=constant_search_iters)\n",
        "    for mu in grid_mu:\n",
        "        pga_model_mom.Mu = mu  # Changing the model hyperparameter\n",
        "        for beta in grid_beta:\n",
        "            pga_model_mom.beta = beta  # Changing the model hyperparameter\n",
        "            print(\"mu:\",round(mu.item(),2),\"beta:\",round(beta.item(),2))\n",
        "            # for a given combination of Mu and Beta, we run the PGA + M algorithm to find what combination brings the best R.\n",
        "            R,_,_ = pga_model_mom.batch_rate(H_valid, W_1, X_1, A, p_s, n_0)\n",
        "            if abs(R[-1]) > abs(best_R):  # Checking if the last R is better than the best R:\n",
        "                best_R = R[-1]\n",
        "                best_mu = mu\n",
        "                best_beta = beta\n",
        "    return best_mu,best_beta, best_R\n",
        "\n",
        "\n",
        "def plot_learning_curve(train_rates, validation_rates):\n",
        "    \"\"\" Plotting the improvment of the sum-rate after training the model for each epoch. Helps us avoid overfitting, tune the learning rate, etc.\n",
        "     Used for internal analysis. \"\"\"\n",
        "    iters_per_epoch = np.ceil(train_size / batch_size)\n",
        "    plt.figure()\n",
        "    y_t = [r.cpu().detach().numpy() for r in train_rates]\n",
        "    x_t = np.array(list(range(len(train_rates)))) / iters_per_epoch\n",
        "    y_v = [r[-1].cpu().detach().numpy() for r in validation_rates]\n",
        "    x_v = np.array(list(range(len(validation_rates)))) + 1\n",
        "    plt.plot(x_t, y_t, 'o', label='Train')\n",
        "    plt.plot(x_v, y_v, '*', label='Valid')\n",
        "    plt.grid()\n",
        "    title = 'Channel Rate After ' + str(J) + ' Iterations Unfolded PGA, After training ' + str(epochs) + ' Epochs \\n ' + \\\n",
        "            W_is_block_diag * 'W is Block Diagonal' + 'W is not Block Diagonal' * (not W_is_block_diag) + \\\n",
        "            ', W is Only Phases' * W_is_phase_only + ', W is not Only Phases' * (not W_is_phase_only) + \\\n",
        "            ', X is variable' * X_is_variable + ', X is fixed' * (not X_is_variable)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(\"Bits per channel use\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    return\n",
        "\n",
        "\n",
        "def plot_comparison(J, validation_rate, pga_iters, Rs_pga, labels = ['Unfolded PGA + M','PGA + M']):\n",
        "    \"\"\" Plotting the calculted rate after each iteration, only for the first J iterations (usually, J=10).\"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(list(range(1,J+1)) , validation_rate.detach().cpu().numpy(), label=labels[0])\n",
        "    plt.plot(list(range(1,pga_iters+1)) , Rs_pga.detach().cpu().numpy(), label=labels[1])\n",
        "    plt.grid()\n",
        "    plt.title('Channel Rate After ' + str(J) +\n",
        "              ' Iterations Unfolded PGA, After training ' + str(epochs) + ' Epochs \\n' +\n",
        "              'with K = ' + str(K))\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel(\"Bits per channel use per user\")\n",
        "    plt.axhline(y=max(abs(Rs_pga)).detach().cpu().numpy(), color='black', linestyle='--', label='Max rate after {} iters'.format(pga_iters))\n",
        "    plt.legend()\n",
        "    plt.xlim(1,J)\n",
        "    # plt.show()\n",
        "    return\n",
        "\n",
        "\n",
        "def plot_comparison_full(J, validation_rate, pga_iters, Rs_pga, labels = ['Unfolded PGA + M','PGA + M']):\n",
        "    \"\"\" Plotting the calculted rate after each iteration, For the whole range of iterations (usually, 400).\"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(list(range(1,J+1)) , validation_rate.detach().cpu().numpy(), label=labels[0])\n",
        "    plt.plot(list(range(1,pga_iters+1)) , Rs_pga.detach().cpu().numpy(), label=labels[1])\n",
        "    plt.grid()\n",
        "    plt.title('Channel Rate After ' + str(J) +\n",
        "              ' Iterations Unfolded PGA, After training ' + str(epochs) + ' Epochs \\n' +\n",
        "              'with K = ' + str(K))\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel(\"Bits per channel use per user\")\n",
        "    plt.axhline(y=max(abs(Rs_pga)).detach().cpu().numpy(), color='black', linestyle='--', label='Max rate after {} iters'.format(pga_iters))\n",
        "    plt.legend()\n",
        "    # plt.show()\n",
        "    return\n",
        "\n",
        "\n",
        "def plot_Mu_W(Mu_W, J):\n",
        "    \"\"\" Plotting the Mu_W matrix, the step size in each iteration.\n",
        "     The functions is used for internal analysis. \"\"\"\n",
        "    fig1, axs1 = plt.subplots(1, J, figsize=(18, 4.5))\n",
        "    # Plotting each slice of Mu_W with the same colorbar range :\n",
        "    global_min = np.min(np.real(Mu_W[:, :, :].cpu().detach().numpy()))\n",
        "    global_max = np.max(np.real(Mu_W[:, :, :].cpu().detach().numpy()))\n",
        "    for j in range(J):\n",
        "        im1 = axs1[j].imshow(np.real(Mu_W[j, :, :].cpu().detach().numpy()), vmin=global_min, vmax=global_max)\n",
        "        axs1[j].set_title('j=' + str(j), fontsize=14)\n",
        "    # Addding a global colorbar for all the plots:\n",
        "    fig1.colorbar(im1, ax=axs1[:], location='bottom')\n",
        "    plt.suptitle('Mu_W', fontsize=28)\n",
        "    plt.show()\n",
        "    return\n",
        "\n",
        "\n",
        "def plot_betas(betas, J):\n",
        "    \"\"\" Plotting the Betas matrix, used for the momentum step.\n",
        "     The function is used for internal analysis. \"\"\"\n",
        "    fig1, axs1 = plt.subplots(1, J, figsize=(18, 4.5))\n",
        "    global_min = np.min(np.real(betas[:, :, :].cpu().detach().numpy()))\n",
        "    global_max = np.max(np.real(betas[:, :, :].cpu().detach().numpy()))\n",
        "    # Plot each slice of Mu_W with the same colorbar range\n",
        "    for j in range(J):\n",
        "        im1 = axs1[j].imshow(np.real(betas[j, :, :].cpu().detach().numpy()), vmin=global_min, vmax=global_max)\n",
        "        axs1[j].set_title('j=' + str(j), fontsize=14)\n",
        "    # Add a global colorbar for all the plots\n",
        "    fig1.colorbar(im1, ax=axs1[:], location='bottom')\n",
        "    plt.suptitle('Momentum Coeffcients', fontsize=28)\n",
        "    plt.show()\n",
        "    return\n",
        "\n",
        "\n",
        "def plot_all(train_rates, validation_rates, Rs_pga, Mu_W, J, pga_iters, to_plot_betas = False, betas = None):\n",
        "    \"\"\" Plotting all the relevant graphs for the unfolded PGA algorithm. \"\"\"\n",
        "    plot_learning_curve(train_rates, validation_rates)\n",
        "    plot_comparison(J,validation_rates[-1], pga_iters, Rs_pga)\n",
        "    plot_comparison_full(J,validation_rates[-1], pga_iters, Rs_pga)\n",
        "    plot_Mu_W(Mu_W, J)\n",
        "    if to_plot_betas:\n",
        "        plot_betas(betas, J)\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Time measurment:"
      ],
      "metadata": {
        "id": "bgJosxkxK2bm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### on CPU:"
      ],
      "metadata": {
        "id": "VFvAqJneLLGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device, n_0, p_s, B, K, P, N, M, L, T, snr, T_opt, A, Ik, Im,D, J, epochs, W_is_block_diag, W_is_phase_only, X_is_variable,\\\n",
        "    train_size, batch_size, pga_iters, optimizer_learning_rate= set_general_params(J=10, epochs=1, batch_size=1,\n",
        "                        T=5,K=20,P=2,L=4,N=8,B=4,train_size = 10, pga_iters = 400,optimizer_learning_rate=0.9)\n",
        "H_train,H_valid,X,W = gen_data(seed = 1, train_size = train_size, valid_size = 1000)\n",
        "H_train,H_valid,X,W = H_train.to(device),H_valid.to(device),X.to(device),W.to(device)\n",
        "grid_line = torch.cat((torch.linspace(0.1, 1, 3),torch.linspace(1.5, 9.5, 17))).to(device)\n",
        "\n",
        "momentum_model, mom_train_rates,mom_validation_rates = train_momentum(H_train,H_valid, X,\n",
        "                                                  W, A, p_s=p_s, n_0=n_0, J=J,Mu_X_init=1.5, Mu_W_init=1.0,\n",
        "                                                  optimizer_learning_rate=optimizer_learning_rate,\n",
        "                                                  epochs=epochs,train_size = train_size,\n",
        "                                                  batch_size = batch_size, betas_init = 0.9)\n",
        "\n",
        "with torch.no_grad():\n",
        "    init_time = time.time()\n",
        "    _ = momentum_model.batch_rate(H_valid,W=W,X=X,A=A,p_s=p_s,n_0=n_0)\n",
        "    final_time = time.time()\n",
        "    print('U - PGA + M time: ', round(1000*(final_time - init_time)/H_valid.shape[0]), 'ms')\n",
        "\n",
        "\n",
        "init_time = time.time()\n",
        "Rs_pga_10,_,_ = run_PGA_Momentum(PGA_Mu_init=1.025,beta=0.911, pga_iters=J,H_valid = H_valid,W=W,X=X,A=A, p_s= p_s, n_0=n_0)\n",
        "final_time = time.time()\n",
        "print('PGA + M : 10 iterations time: ', round(1000*(final_time - init_time)/H_valid.shape[0]), 'ms')\n",
        "\n",
        "\n",
        "init_time = time.time()\n",
        "Rs_line_search, _ = line_search(H_valid, X, W, A, p_s, n_0, grid_line, line_search_iters = 50)\n",
        "final_time = time.time()\n",
        "print('Line Search time: ', round(1000*(final_time - init_time)/H_valid.shape[0]), 'ms')\n",
        "\n",
        "\n",
        "init_time = time.time()\n",
        "Rs_pga_400, _,_ = run_PGA_Momentum(PGA_Mu_init=1.0, beta=0.9, pga_iters=pga_iters,H_valid = H_valid,W=W,X=X,A=A, p_s= p_s, n_0=n_0)\n",
        "final_time = time.time()\n",
        "print('PGA + M : 400 iterations time: ', round(1000*(final_time - init_time)/H_valid.shape[0]), 'ms')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1P3E6HRK065",
        "outputId": "2e1c45fd-1d1a-4894-e0d2-07d79019f898"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Momentum starts training\n",
            "epoch:  1  of  1\n",
            "epoch time:  0.7  seconds\n",
            "U - PGA + M time:  24 ms\n",
            "PGA + M : 10 iterations time:  24 ms\n",
            "Line Search time:  1133 ms\n",
            "PGA + M : 400 iterations time:  928 ms\n"
          ]
        }
      ]
    }
  ]
}